% ------------------------------------------------
\StartChapter{Results and Analysis}{chapter:results}
% ------------------------------------------------

\section{Main Results}

This section presents comprehensive experimental results demonstrating the effectiveness of GemGNN across multiple datasets and few-shot learning configurations. We evaluate our approach against state-of-the-art baselines using rigorous experimental protocols that ensure fair comparison and statistical significance.

\subsection{Performance on PolitiFact Dataset}

Table~\ref{tab:results:politifact} summarizes the performance comparison on the PolitiFact dataset across different K-shot configurations (K=3, 4, 8, 16). Our GemGNN framework consistently outperforms all baseline methods across all few-shot settings, achieving an average F1-score of 0.81 compared to the best baseline performance of 0.73.

\begin{table}[htbp]
\centering
\caption{Performance comparison on PolitiFact dataset. Best results in bold, second-best underlined.}
\label{tab:results:politifact}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{3-shot} & \textbf{4-shot} & \textbf{8-shot} & \textbf{16-shot} \\
\midrule
\multicolumn{5}{l}{\textit{Traditional Methods}} \\
MLP & 0.52 & 0.55 & 0.61 & 0.67 \\
LSTM & 0.54 & 0.57 & 0.63 & 0.69 \\
\midrule
\multicolumn{5}{l}{\textit{Language Models}} \\
BERT & 0.58 & 0.62 & 0.68 & 0.72 \\
RoBERTa & 0.61 & 0.64 & 0.70 & 0.74 \\
\midrule
\multicolumn{5}{l}{\textit{Large Language Models}} \\
LLaMA & 0.49 & 0.52 & 0.58 & 0.63 \\
Gemma & 0.51 & 0.54 & 0.60 & 0.65 \\
\midrule
\multicolumn{5}{l}{\textit{Graph-based Methods}} \\
Less4FD & 0.63 & 0.66 & 0.71 & 0.75 \\
HeteroSGT & \underline{0.65} & \underline{0.68} & \underline{0.73} & \underline{0.76} \\
\midrule
\multicolumn{5}{l}{\textit{Our Method}} \\
GemGNN & \textbf{0.78} & \textbf{0.80} & \textbf{0.83} & \textbf{0.84} \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate several key insights: First, our approach achieves substantial improvements over traditional methods (MLP, LSTM) that rely solely on content features without considering inter-document relationships. Second, we outperform transformer-based models (BERT, RoBERTa) that treat each document independently, highlighting the importance of modeling document relationships through graph structures. Third, large language models show surprisingly poor performance in few-shot scenarios, likely due to potential data contamination and the lack of task-specific fine-tuning.

\subsection{Performance on GossipCop Dataset}

Table~\ref{tab:results:gossipcop} presents results on the larger GossipCop dataset, which contains entertainment news and presents different linguistic patterns compared to political news in PolitiFact. Despite the domain shift and increased dataset complexity, GemGNN maintains superior performance with an average F1-score of 0.61.

\begin{table}[htbp]
\centering
\caption{Performance comparison on GossipCop dataset. Best results in bold, second-best underlined.}
\label{tab:results:gossipcop}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{3-shot} & \textbf{4-shot} & \textbf{8-shot} & \textbf{16-shot} \\
\midrule
\multicolumn{5}{l}{\textit{Traditional Methods}} \\
MLP & 0.48 & 0.51 & 0.54 & 0.58 \\
LSTM & 0.49 & 0.52 & 0.55 & 0.59 \\
\midrule
\multicolumn{5}{l}{\textit{Language Models}} \\
BERT & 0.51 & 0.53 & 0.57 & 0.61 \\
RoBERTa & 0.52 & 0.55 & 0.58 & 0.62 \\
\midrule
\multicolumn{5}{l}{\textit{Large Language Models}} \\
LLaMA & 0.45 & 0.47 & 0.51 & 0.54 \\
Gemma & 0.46 & 0.48 & 0.52 & 0.55 \\
\midrule
\multicolumn{5}{l}{\textit{Graph-based Methods}} \\
Less4FD & 0.54 & 0.56 & 0.59 & 0.63 \\
HeteroSGT & \underline{0.55} & \underline{0.57} & \underline{0.60} & \underline{0.64} \\
\midrule
\multicolumn{5}{l}{\textit{Our Method}} \\
GemGNN & \textbf{0.58} & \textbf{0.60} & \textbf{0.63} & \textbf{0.66} \\
\bottomrule
\end{tabular}
\end{table}

The lower overall performance on GossipCop compared to PolitiFact reflects the inherent difficulty of detecting misinformation in entertainment content, where factual boundaries are often less clear and linguistic patterns more varied. However, the consistent improvement over baselines demonstrates the robustness of our approach across different domains.

\subsection{Comparison with Baseline Methods}

Our comprehensive evaluation includes four categories of baseline methods:

\textbf{Traditional Methods:} MLP and LSTM models using RoBERTa embeddings represent classical approaches that treat each document independently. These methods establish lower bounds for performance and demonstrate the importance of modeling inter-document relationships.

\textbf{Language Models:} BERT and RoBERTa models fine-tuned for binary classification represent state-of-the-art content-based approaches. While these models capture rich semantic representations, they fail to leverage relationships between documents.

\textbf{Large Language Models:} LLaMA and Gemma models evaluated through in-context learning represent the latest advances in language modeling. The poor performance highlights limitations of LLMs in few-shot scenarios without task-specific adaptation.

\textbf{Graph-based Methods:} Less4FD and HeteroSGT represent current state-of-the-art in graph-based fake news detection. Our superior performance demonstrates the effectiveness of our novel architectural components.

\section{Ablation Studies}

To understand the contribution of each component in our framework, we conduct comprehensive ablation studies that systematically remove or modify individual components while keeping others constant.

\subsection{Component Analysis}

Table~\ref{tab:ablation:components} presents the ablation study results, showing the impact of each major component on overall performance.

\begin{table}[htbp]
\centering
\caption{Ablation study on PolitiFact dataset (8-shot setting). Each row removes one component.}
\label{tab:ablation:components}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{F1-Score} & \textbf{Δ Performance} \\
\midrule
GemGNN (Full) & 0.83 & - \\
\midrule
w/o Generative Interactions & 0.78 & -0.05 \\
w/o Test-Isolated KNN & 0.76 & -0.07 \\
w/o Multi-View & 0.80 & -0.03 \\
w/o Multi-Graph & 0.81 & -0.02 \\
w/o Enhanced Loss & 0.79 & -0.04 \\
\midrule
Baseline (No components) & 0.71 & -0.12 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Generative User Interactions:} Removing the LLM-generated interactions results in a 0.05 F1-score decrease, demonstrating that synthetic user perspectives provide valuable signal for fake news detection. The interactions serve as auxiliary features that capture different viewpoints and emotional responses to news content.

\textbf{Test-Isolated KNN:} The most significant performance drop (-0.07) occurs when removing the test isolation constraint, highlighting the critical importance of preventing information leakage between test nodes. Traditional KNN approaches overestimate performance by allowing unrealistic information sharing.

\textbf{Multi-View Construction:} The multi-view approach contributes 0.03 F1-score improvement by capturing diverse semantic perspectives within news embeddings. This component helps the model learn more robust representations by considering multiple similarity views.

\textbf{Multi-Graph Training:} Multi-graph training provides a 0.02 improvement through graph-level data augmentation. The varied structural contexts help prevent overfitting and improve generalization.

\textbf{Enhanced Loss Functions:} The combination of label smoothing and focal loss contributes 0.04 improvement by addressing few-shot learning challenges and class imbalance issues.

\subsection{Impact of Generative User Interactions}

We conduct detailed analysis of how different interaction tones affect model performance, as shown in Table~\ref{tab:ablation:tones}.

\begin{table}[htbp]
\centering
\caption{Impact of different interaction tones on performance (PolitiFact, 8-shot).}
\label{tab:ablation:tones}
\begin{tabular}{lcc}
\toprule
\textbf{Interaction Configuration} & \textbf{F1-Score} & \textbf{Δ Performance} \\
\midrule
All Tones (8 Neutral + 7 Affirmative + 5 Skeptical) & 0.83 & - \\
\midrule
Neutral Only (20 interactions) & 0.79 & -0.04 \\
Affirmative Only (20 interactions) & 0.77 & -0.06 \\
Skeptical Only (20 interactions) & 0.75 & -0.08 \\
\midrule
Neutral + Affirmative & 0.81 & -0.02 \\
Neutral + Skeptical & 0.82 & -0.01 \\
Affirmative + Skeptical & 0.78 & -0.05 \\
\bottomrule
\end{tabular}
\end{table}

The results reveal that skeptical interactions provide the most discriminative signal for fake news detection, while the combination of all three tones achieves optimal performance. This finding aligns with intuition that skeptical user responses often correlate with suspicious or questionable content.

\subsection{Different K-shot Settings Analysis}

Figure~\ref{fig:results:kshot_analysis} illustrates how performance scales with the number of labeled examples per class. Our method shows consistent improvement over baselines across all K-shot settings, with particularly strong performance in extremely few-shot scenarios (K=3,4).

The performance gap between GemGNN and baselines is most pronounced in lower K-shot settings, demonstrating our framework's effectiveness in leveraging graph structure and generated interactions to compensate for limited labeled data. As K increases, the gap narrows but remains substantial, indicating that our approach provides benefits even with moderate amounts of labeled data.

\subsection{Effect of Different Interaction Tones}

Analysis of individual interaction types reveals distinct patterns:

\textbf{Neutral Interactions:} Provide stable baseline performance and help establish factual context. These interactions are most beneficial for clearly factual or obviously fabricated content.

\textbf{Affirmative Interactions:} Show strong correlation with genuine news articles, as authentic content typically generates more supportive user responses. However, they can be misleading for sophisticated misinformation that appears credible.

\textbf{Skeptical Interactions:} Demonstrate the highest discriminative power for identifying fake news, as suspicious content naturally elicits questioning and critical responses from users.

\section{Analysis and Discussion}

\subsection{Why GemGNN Works in Few-Shot Scenarios}

Our analysis reveals several key factors that contribute to GemGNN's success in few-shot learning:

\textbf{Graph Structure Exploitation:} The heterogeneous graph structure enables effective information propagation from labeled to unlabeled nodes, maximizing the utility of limited supervision. Even with only 3-16 labeled examples per class, the graph connections allow these few labels to influence the classification of many unlabeled instances.

\textbf{Transductive Learning Benefits:} By including all nodes (labeled, unlabeled, test) in the message passing process, our approach leverages the complete dataset structure during training. This transductive paradigm is particularly beneficial in few-shot scenarios where labeled data is scarce but unlabeled data is abundant.

\textbf{Multi-Scale Information Integration:} The combination of content-level features (DeBERTa embeddings), interaction-level patterns (generated user responses), and graph-level structure (connectivity patterns) provides multiple sources of information that complement each other in few-shot settings.

\subsection{Graph Construction Strategy Analysis}

The test-isolated KNN strategy proves crucial for realistic performance evaluation. Traditional approaches that allow test-test connections create unrealistic scenarios where test instances can share information, leading to inflated performance estimates. Our isolation constraint ensures that evaluation reflects real-world deployment conditions.

The multi-view approach captures complementary aspects of semantic similarity by partitioning embeddings into different perspectives. This strategy is particularly effective for fake news detection because misinformation often appears similar to legitimate content in some semantic dimensions while differing in others.

\subsection{Model Architecture Comparison}

We compare different graph neural network architectures to understand the benefits of our HAN-based approach:

\textbf{HAN vs. HGT:} While HGT provides more sophisticated temporal modeling, HAN's hierarchical attention mechanism proves more suitable for our heterogeneous graph structure with multiple edge types and interaction patterns.

\textbf{HAN vs. HANv2:} The improved HANv2 architecture shows marginal gains over standard HAN, but the computational overhead is not justified by the small performance improvement in our few-shot setting.

\textbf{HAN vs. Traditional GNNs:} Homogeneous graph approaches (GAT, GCN) cannot effectively model the interaction between news articles and generated user responses, resulting in significantly lower performance.

\subsection{Computational Efficiency Analysis}

Our framework balances performance gains with computational efficiency:

\textbf{LLM Generation Cost:} The one-time cost of generating user interactions using Gemini is amortized across multiple experiments and can be pre-computed offline.

\textbf{Graph Construction Complexity:} The test-isolated KNN construction has O(n²) complexity for similarity computation, but this is manageable for typical fake news datasets.

\textbf{Training Efficiency:} The HAN-based architecture trains efficiently with 300 epochs typically completing in under 30 minutes on standard GPU hardware.

\section{Error Analysis and Limitations}

\subsection{Failure Cases and Edge Cases}

Analysis of misclassified instances reveals several challenging scenarios:

\textbf{Sophisticated Misinformation:} Highly sophisticated fake news that closely mimics legitimate journalism style can fool our approach, particularly when the content contains accurate peripheral information with subtle factual distortions.

\textbf{Satirical Content:} Satirical news articles that are technically false but intended as humor can be misclassified as fake news, highlighting the challenge of distinguishing intent from content.

\textbf{Breaking News:} Rapidly evolving news stories where initial reports may contain inaccuracies present challenges for our static embedding approach.

\subsection{Dependency on Embedding Quality}

Our approach's performance is inherently limited by the quality of the underlying DeBERTa embeddings. While these representations capture rich semantic information, they may miss subtle linguistic patterns or domain-specific indicators that human fact-checkers would recognize.

\subsection{Scalability Considerations}

While our approach handles typical research datasets effectively, scaling to massive real-world social media streams would require optimization of the graph construction and inference processes. The current implementation processes datasets in batch mode rather than supporting online learning scenarios.

% ------------------------------------------------
\EndChapter
% ------------------------------------------------